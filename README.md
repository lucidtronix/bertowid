# BERTowid
This repo contains the code for the paper ["Language Models Learn Sentiment and Substance from 11,000 Psychoactive Experiences"](https://www.biorxiv.org/content/10.1101/2022.06.02.494544)

<img width="1505" alt="Screen Shot 2022-06-30 at 6 37 36 AM" src="https://user-images.githubusercontent.com/2604962/176900037-cc66bbf8-af9d-4284-a76e-f51cdcefbcce.png">
This figure represents an overview of the study showing data sources, models and a selection of key results. Data from Wikipedia, IMDB, Rotten Tomatoes, Reddit, Erowid, and the PDSP  was used in pre-training and training the 3 models: BERTowid, BERTiment, and CCA. A) BERTiment’s concordance with a clinical-psychiatrist emotion adjudication in Erowid testimonials. B) IMDB movie review hedonic-tone classifier correlation with the 28 emotions annotated in Reddit.  In C)-G) we show BERTowid output heads evaluated on held-out test set testimonials: C) BERTowid psychoactive classification, mean precision per class. D) Self-reported gender classification ROC curve. E) Predict age from testimonials F) Predict CCA factors per-testimonial weighting for each of the 11 axes of the receptor-experience space. G) Regress receptor affinity directly from testimonial text.  Tiling inferences from BERT models along the narrative of the testimonials we construct trajectories, for example, H) shows BERTiment’s predicted emotion: Love for select drugs over the course of a trip (note the prominence of MDMA).  I) BERTowid’s predicted trajectories for the semantic tag: Mystical Experience (note the prominence of DMT).
